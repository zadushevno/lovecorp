{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from string import punctuation\n",
    "from nltk import sent_tokenize, wordpunct_tokenize\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Если кто-либо достоинНеземной любви,То я знаю,...</td>\n",
       "      <td>stihi</td>\n",
       "      <td>https://skynight.ru/stihi_love.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Спасибо за то, что ты есть.За то, что твой гол...</td>\n",
       "      <td>stihi</td>\n",
       "      <td>https://skynight.ru/stihi_love.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Среди таинственно-высоких гор,Вокруг кристальн...</td>\n",
       "      <td>stihi</td>\n",
       "      <td>https://skynight.ru/stihi_love.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Когда меня ты нежно обнимаешь,Я тут же забываю...</td>\n",
       "      <td>stihi</td>\n",
       "      <td>https://skynight.ru/stihi_love.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Люблю, скучаю, обожаю,Схожу с ума и умираюОт с...</td>\n",
       "      <td>stihi</td>\n",
       "      <td>https://skynight.ru/stihi_love.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19015</th>\n",
       "      <td>19023</td>\n",
       "      <td>– Послушай, я люблю тебя больше всех на свете....</td>\n",
       "      <td>book</td>\n",
       "      <td>Стефани Майер, \"Сумерки\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19016</th>\n",
       "      <td>19024</td>\n",
       "      <td>– Да, – улыбнулся Эдвард. – Достаточно на сего...</td>\n",
       "      <td>book</td>\n",
       "      <td>Стефани Майер, \"Сумерки\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19017</th>\n",
       "      <td>19025</td>\n",
       "      <td>Холодные губы снова прильнули к моей шее.</td>\n",
       "      <td>book</td>\n",
       "      <td>Стефани Майер, \"Сумерки\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19018</th>\n",
       "      <td>19026</td>\n",
       "      <td>1</td>\n",
       "      <td>book</td>\n",
       "      <td>Стефани Майер, \"Сумерки\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19019</th>\n",
       "      <td>19027</td>\n",
       "      <td>Франческо Солимена (1657–1747) – итальянский ж...</td>\n",
       "      <td>book</td>\n",
       "      <td>Стефани Майер, \"Сумерки\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19020 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                               text  topic  \\\n",
       "0          0  Если кто-либо достоинНеземной любви,То я знаю,...  stihi   \n",
       "1          1  Спасибо за то, что ты есть.За то, что твой гол...  stihi   \n",
       "2          2  Среди таинственно-высоких гор,Вокруг кристальн...  stihi   \n",
       "3          3  Когда меня ты нежно обнимаешь,Я тут же забываю...  stihi   \n",
       "4          4  Люблю, скучаю, обожаю,Схожу с ума и умираюОт с...  stihi   \n",
       "...      ...                                                ...    ...   \n",
       "19015  19023  – Послушай, я люблю тебя больше всех на свете....   book   \n",
       "19016  19024  – Да, – улыбнулся Эдвард. – Достаточно на сего...   book   \n",
       "19017  19025          Холодные губы снова прильнули к моей шее.   book   \n",
       "19018  19026                                                  1   book   \n",
       "19019  19027  Франческо Солимена (1657–1747) – итальянский ж...   book   \n",
       "\n",
       "                                    source  \n",
       "0      https://skynight.ru/stihi_love.html  \n",
       "1      https://skynight.ru/stihi_love.html  \n",
       "2      https://skynight.ru/stihi_love.html  \n",
       "3      https://skynight.ru/stihi_love.html  \n",
       "4      https://skynight.ru/stihi_love.html  \n",
       "...                                    ...  \n",
       "19015             Стефани Майер, \"Сумерки\"  \n",
       "19016             Стефани Майер, \"Сумерки\"  \n",
       "19017             Стефани Майер, \"Сумерки\"  \n",
       "19018             Стефани Майер, \"Сумерки\"  \n",
       "19019             Стефани Майер, \"Сумерки\"  \n",
       "\n",
       "[19020 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''функция для разлипания строк и нормальной построчной записи стихов, а еще для разлипания знаков препинания'''\n",
    "    pattern1 = r'([.,!?–:;-]|[а-я])([А-Я])'\n",
    "    repl1 = r'\\1\\n\\2'\n",
    "    #pattern2 = r'([.,!?–:;])(\\S)'\n",
    "    repl2 = r'\\1 \\2'\n",
    "    pattern3 = r'(\\.{3})([–.,!?–:;А-Я])'\n",
    "    res = re.sub(pattern1, repl1, text)\n",
    "    res = re.sub(pattern3, repl1, res)\n",
    "    #res = re.sub(pattern2, repl2, res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для препроцессинга (очистка от слипшихся символов и деление на предложения и строки)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    text = df['text']\n",
    "    source = df['source']\n",
    "    topic = df['topic']\n",
    "    sentences_cl = text.split('\\n')\n",
    "    sentences = []\n",
    "    for sent in sentences_cl:\n",
    "        sent = clean_text(sent)\n",
    "        pattern2 = r'([.,!?–:;…])(\\S)'\n",
    "        repl2 = r'\\1\\n\\2'\n",
    "        res = re.sub(pattern2, repl2, sent)\n",
    "        sent = res.split('\\n')\n",
    "        for s in sent:\n",
    "            sentences.extend(sent_tokenize(s))\n",
    "    return (clean_text(text), sentences, topic, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для морфологической разметки и записи метаинформации о тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag(df, i):\n",
    "    info = {}\n",
    "    info['index'] = i\n",
    "    prep = preprocessing(df)\n",
    "    info['text'] = prep[0]\n",
    "    info['source'] = prep[3]\n",
    "    info['topic'] = prep[2]\n",
    "    sentences = prep[1]\n",
    "    text_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence != '..':\n",
    "            sent_info = {}\n",
    "            sent_info['sentence'] = sentence\n",
    "            doc = nlp(sentence)\n",
    "            words = []\n",
    "            for token in doc:\n",
    "                if token.pos_ != 'PUNCT' and token.text.isalpha(): #проверяем токен\n",
    "                    word_info = {}\n",
    "                    word_info['word'] = token.text.lower()\n",
    "                    word_info['lemma'] = token.lemma_\n",
    "                    if token.pos_ == 'AUX': #убираем теги AUX\n",
    "                        word_info['pos'] = 'VERB'\n",
    "                    else:\n",
    "                        word_info['pos'] = token.pos_\n",
    "                    words.append(word_info)\n",
    "            sent_info['words'] = words\n",
    "            text_sentences.append(sent_info)\n",
    "    info['sentences'] = text_sentences\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data.index)):\n",
    "    try:\n",
    "        analysis.append(tag(data.iloc[i], i))\n",
    "    except:\n",
    "        print(data.iloc[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "создаем базу данных: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpname = 'LOVECORP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(f'{corpname}.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS texts\n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, text text, topic text, source text)\n",
    "\"\"\")\n",
    "# таблица тексты: ключ, текст, источник\n",
    "# таблица предложения: номер текста, текст предложения\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sentences \n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, text_id int, sentence text) \n",
    "    \"\"\")\n",
    "# таблика токен: позиция в предложении, номер текста, номер предложения, токен(=слово как встретилось), лемма, часть речи\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS token\n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, text_id int, sentence_id int, token text, lem text, pos text) \n",
    "\"\"\")\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция для записи в БД:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(f'{corpname}.db', timeout=10)\n",
    "cur = conn.cursor()\n",
    "def write_to_db(info):\n",
    "    text = info['text']\n",
    "    text_id = info['index'] + 1\n",
    "    topic = info['topic']\n",
    "    source = info['source']\n",
    "    cur.execute('INSERT INTO texts VALUES (?, ?, ?, ?)', (text_id, text, topic, source))\n",
    "    conn.commit()\n",
    "    sent_id = text_id * 1000 #танцы с бубном, чтобы id не повторялись, но было видно, какое это по счету предложение текста\n",
    "    for item in info['sentences']:\n",
    "        sentence = item['sentence']\n",
    "        cur.execute('INSERT INTO sentences VALUES (?, ?, ?)', (sent_id, text_id, sentence))\n",
    "        conn.commit()\n",
    "        word_num = sent_id * 10000 \n",
    "        for word in item['words']:\n",
    "            token = word['word']\n",
    "            lemma = word['lemma']\n",
    "            pos = word['pos']\n",
    "            cur.execute('INSERT INTO token VALUES (?, ?, ?, ?, ?, ?)', (word_num, text_id, sent_id, token, lemma, pos))\n",
    "            conn.commit()\n",
    "            word_num += 1\n",
    "        sent_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "пишем данные в БД:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in analysis:\n",
    "    write_to_db(el)\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
